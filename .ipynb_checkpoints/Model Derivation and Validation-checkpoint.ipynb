{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zKDULv20258W"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import tempfile\n",
    "import pandas as pd\n",
    "import seaborn as sns; sns.set()\n",
    "import sklearn\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import auc\n",
    "import matplotlib.lines as mlines\n",
    "mpl.rcParams['figure.figsize'] = (8, 6)\n",
    "mpl.rcParams['axes.grid'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2D1exv7N3xdt"
   },
   "outputs": [],
   "source": [
    "!pip install -U tensorboard\n",
    "!pip install keras-tuner\n",
    "import kerastuner as kt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nu1xO4w83AO-"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L4lhCXYGLH7R"
   },
   "source": [
    "# Load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZBXspqV_aLJf"
   },
   "outputs": [],
   "source": [
    "tdata_name,tlabels_name,hdata_name,hlabels_name = \"tseq_15Tr10jd.npy\", 'tlabels_15Tr10jd.npy', \"hseq_15Tr10jd.npy\", 'hlabels_15Tr10jd.npy'\n",
    "my_data_dir = 'Colab Notebooks/last3612/'\n",
    "data_training = np.load(\"gdrive/My Drive/\"+my_data_dir + tdata_name)\n",
    "labels_training = np.load(\"gdrive/My Drive/\"+my_data_dir + tlabels_name)\n",
    "\n",
    "data_holdout = np.load(\"gdrive/My Drive/\"+my_data_dir + hdata_name)\n",
    "labels_holdout = np.load(\"gdrive/My Drive/\"+my_data_dir + hlabels_name)\n",
    "\n",
    "#view data\n",
    "print(labels_training.mean(), labels_holdout.mean())\n",
    "print(data_training.shape, data_holdout.shape)\n",
    "\n",
    "pos, neg = sum(labels_training != 0),sum(labels_training == 0)\n",
    "total = len(labels_training)\n",
    "print(pos, neg, total)\n",
    "\n",
    "#\n",
    "pos_data = data_training[labels_training != 0]\n",
    "neg_data = data_training[labels_training == 0]\n",
    "pos_data= np.sum(pos_data,axis=1)\n",
    "neg_data= np.sum(neg_data,axis=1)\n",
    "print(pd.DataFrame({'outcome':pos_data.mean(axis=0),'survival':neg_data.mean(axis=0)}))\n",
    "\n",
    "#calculate weight\n",
    "\n",
    "weight_for_0 = (1 / neg)*(total)/2.0 \n",
    "weight_for_1 = (1 / pos)*(total)/2.0\n",
    "\n",
    "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "print(class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CYX_DRAT7-DP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DcVdo-jhLam9"
   },
   "source": [
    "# Functions for Dataset, Metric, and Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FPG7m9eDH2Mq"
   },
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "def Create_dataset(data, label, BATCH_SIZE = 128):\n",
    "  dataset = tf.data.Dataset.from_tensor_slices((data, label))\n",
    "  dataset = dataset.batch(BATCH_SIZE).prefetch(2)\n",
    "  return dataset\n",
    "\n",
    "# resample\n",
    "#since the dataset is imbalanced, resampling is needed to improve model performance\n",
    "def Resample_dataset(data, label, BATCH_SIZE = 128,BUFFER_SIZE = 10000):\n",
    "  pos_labels = label[label != 0]\n",
    "  neg_labels = label[label == 0]\n",
    "  pos_data = data[label != 0]\n",
    "  neg_data = data[label == 0]\n",
    "\n",
    "  pos_ds = tf.data.Dataset.from_tensor_slices((pos_data, pos_labels))\n",
    "  pos_ds = pos_ds.cache().shuffle(BUFFER_SIZE).repeat()\n",
    "\n",
    "  neg_ds= tf.data.Dataset.from_tensor_slices((neg_data, neg_labels))\n",
    "  neg_ds = neg_ds.cache().shuffle(BUFFER_SIZE).repeat()\n",
    "\n",
    "  resampled_ds = tf.data.experimental.sample_from_datasets([pos_ds, neg_ds], weights=[0.5, 0.5], seed=0)\n",
    "  resampled_ds = resampled_ds.batch(BATCH_SIZE).prefetch(2)\n",
    "  return resampled_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TfSvZe2k6ZJT"
   },
   "outputs": [],
   "source": [
    "METRICS = [\n",
    "      tf.keras.metrics.BinaryAccuracy(name='accuracy',threshold=0.5),\n",
    "      tf.keras.metrics.AUC(name='auc'),\n",
    "      tf.keras.metrics.Precision(name='precision',thresholds=0.5),\n",
    "      tf.keras.metrics.Recall(name='recall',thresholds=0.5),\n",
    "      tf.keras.metrics.TruePositives(name='tp'),\n",
    "      tf.keras.metrics.FalsePositives(name='fp'),\n",
    "      tf.keras.metrics.TrueNegatives(name='tn'),\n",
    "      tf.keras.metrics.FalseNegatives(name='fn'), \n",
    "]\n",
    "\n",
    "EPOCHS = 1000\n",
    "BATCH_SIZE = 128\n",
    "resampled_steps_per_epoch = np.ceil(2.0*neg/BATCH_SIZE)\n",
    "print(resampled_steps_per_epoch)\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_auc', \n",
    "    verbose=1,\n",
    "    patience=10,\n",
    "    mode='max',\n",
    "    restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0ArtLQ_fJCCG"
   },
   "outputs": [],
   "source": [
    "# Plots for model validation and diagnosis. \n",
    "def plot_metrics(history):\n",
    "  metrics =  ['loss', 'auc', 'precision', 'recall']\n",
    "  for n, metric in enumerate(metrics):\n",
    "    name = metric.replace(\"_\",\" \").capitalize()\n",
    "    plt.subplot(2,2,n+1)\n",
    "    plt.plot(history.epoch,  history.history[metric], label='Train')\n",
    "    plt.plot(history.epoch, history.history['val_'+metric],\n",
    "              linestyle=\"--\", label='Val')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(name)\n",
    "    if metric == 'loss':\n",
    "      plt.ylim([0, plt.ylim()[1]])\n",
    "    elif metric == 'auc':\n",
    "      plt.ylim([0.6,1])\n",
    "    else:\n",
    "      plt.ylim([0,1])\n",
    "    plt.legend()\n",
    "\n",
    "def plot_roc(name, labels, predictions, **kwargs):\n",
    "  fp, tp, _ = sklearn.metrics.roc_curve(labels, predictions)\n",
    "\n",
    "  plt.plot(100*fp, 100*tp, label=name, linewidth=2, **kwargs)\n",
    "  plt.xlabel('False positives [%]')\n",
    "  plt.ylabel('True positives [%]')\n",
    "  #plt.xlim([-0.5,40])\n",
    "  #plt.ylim([60,100.5])\n",
    "  plt.grid(True)\n",
    "  ax = plt.gca()\n",
    "  ax.set_aspect('equal')\n",
    "\n",
    "def plot_cm(labels, predictions, p=0.5):\n",
    "  cm = confusion_matrix(labels, predictions > p)\n",
    "  plt.figure(figsize=(5,5))\n",
    "  sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "  plt.title('Confusion matrix @{:.2f}'.format(p))\n",
    "  plt.ylabel('Actual label')\n",
    "  plt.xlabel('Predicted label')\n",
    "\n",
    "  print('Legitimate Transactions Detected (True Negatives): ', cm[0][0])\n",
    "  print('Legitimate Transactions Incorrectly Detected (False Positives): ', cm[0][1])\n",
    "  print('Fraudulent Transactions Missed (False Negatives): ', cm[1][0])\n",
    "  print('Fraudulent Transactions Detected (True Positives): ', cm[1][1])\n",
    "  print('Total Fraudulent Transactions: ', np.sum(cm[1]))\n",
    "\n",
    "def plot_pr_curve(name, labels, predictions, **kwargs):\n",
    "  precision, recall, thresholds = precision_recall_curve(labels, predictions)\n",
    "  print('precision',precision)\n",
    "  print('recall',recall)\n",
    "  print('thresholds',thresholds)\n",
    "  auc_score = auc(recall, precision)\n",
    "  print('PR AUC',auc_score)\n",
    "  plt.plot(100*recall, 100*precision, label=name, linewidth=2, **kwargs)\n",
    "  plt.xlabel('recall [%]')\n",
    "  plt.ylabel('precision [%]')\n",
    "  #plt.xlim([-0.5,40])\n",
    "  #plt.ylim([60,100.5])\n",
    "  plt.grid(True)\n",
    "  ax = plt.gca()\n",
    "  ax.legend()\n",
    "  ax.set_aspect('equal')\n",
    "\n",
    "def plot_calibration(name, labels, predictions, **kwargs):\n",
    "  y, x = calibration_curve(labels, predictions, n_bins=20)\n",
    "  # calibration curves\n",
    "  plt.figure(figsize=(10, 10))\n",
    "  ax1 = plt.subplot2grid((3, 1), (0, 0), rowspan=2)\n",
    "  ax2 = plt.subplot2grid((3, 1), (2, 0))\n",
    "  ax1.plot(x, y, \"s-\",label=\"%s\" % (name, ))\n",
    "  line = mlines.Line2D([0, 1], [0, 1], color='black')\n",
    "  #transform = ax.transAxes\n",
    "  #line.set_transform(transform)\n",
    "  ax1.add_line(line)\n",
    "  ax2.hist(predictions, range=(0, 1), bins=20, label=name, histtype=\"step\", lw=2)\n",
    "\n",
    "  ax1.set_ylabel(\"Fraction of positives\")\n",
    "  ax1.set_ylim([-0.05, 1.05])\n",
    "  ax1.legend(loc=\"lower right\")\n",
    "  ax1.set_title('Calibration plots  (reliability curve)')\n",
    "\n",
    "  ax2.set_xlabel(\"Mean predicted value\")\n",
    "  ax2.set_ylabel(\"Count\")\n",
    "  ax2.legend(loc=\"upper center\", ncol=2)\n",
    "\n",
    "  plt.tight_layout()\n",
    "  plt.plot(y, x, marker='o', linewidth=1, label=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "spC5Ngmi8ZDy"
   },
   "source": [
    "# Model Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vB53ipic4c5h"
   },
   "source": [
    "## Keras Tuner\n",
    "Use package kerastuner to tune hyperparameters and find the best deep neural network structure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RFJEX7Er7ICE"
   },
   "outputs": [],
   "source": [
    "#load dataset\n",
    "data_train, data_val, labels_train, labels_val = train_test_split(data_training, labels_training, test_size=0.25, random_state=42)\n",
    "split_point=9\n",
    "julian_training = data_train[:,0,split_point:]\n",
    "julina_holdout = data_holdout[:,0,split_point:]\n",
    "julina_val = data_val[:,0,split_point:]\n",
    "\n",
    "data_train = data_train[:,:,:split_point]\n",
    "data_holdout = data_holdout[:,:,:split_point]\n",
    "data_val = data_val[:,:,:split_point]\n",
    "# SUM\n",
    "data_train_point = np.sum(data_train,axis=1)\n",
    "data_val_point = np.sum(data_val,axis=1)\n",
    "data_test_point = np.sum(data_holdout,axis=1)\n",
    "# concat\n",
    "data_train_point = np.concatenate((data_train_point, julian_training), axis=1) \n",
    "data_val_point = np.concatenate((data_val_point, julina_val), axis=1) \n",
    "data_test_point = np.concatenate((data_test_point, julina_holdout), axis=1)\n",
    "print(data_train_point.shape,data_val_point.shape,data_test_point.shape)\n",
    "\n",
    "\n",
    "valset = Create_dataset(data_val_point, labels_val)\n",
    "testset = Create_dataset(data_test_point, labels_holdout)\n",
    "trainset = Resample_dataset(data_train_point,labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jn2G-Qjs6T9v"
   },
   "source": [
    "Logistic regression\n",
    "Search for best lambda for L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yNcxkZAy2mUn"
   },
   "outputs": [],
   "source": [
    "from kerastuner import HyperModel\n",
    "l= [100.0,50.0,20.0,10.0,5.0,2.0,1.0,0.5,0.01,0.005,0.001]\n",
    "\n",
    "class LRHyperModel(HyperModel):\n",
    "    def __init__(self, input_shape):\n",
    "        self.input_shape = input_shape\n",
    "\n",
    "    def build(self, hp):\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(\n",
    "            tf.keras.layers.Dense(1,\n",
    "                kernel_regularizer=tf.keras.regularizers.l2(\n",
    "                    hp.Choice('lamba',\n",
    "                              values=l,\n",
    "                              default=0.01\n",
    "                    )\n",
    "                ),\n",
    "                activation='sigmoid',\n",
    "                input_shape=self.input_shape\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        model.compile(\n",
    "          optimizer=tf.keras.optimizers.Adam(lr=3e-4),\n",
    "          loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "          metrics=METRICS)\n",
    "        \n",
    "        return model\n",
    "\n",
    "INPUT_SHAPE = (data_train_point.shape[-1],)\n",
    "hypermodel = LRHyperModel(input_shape=INPUT_SHAPE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E1xfYJFm2mUw"
   },
   "outputs": [],
   "source": [
    "\n",
    "HYPERBAND_MAX_EPOCHS = 40\n",
    "SEED=41\n",
    "tuner = kt.Hyperband(\n",
    "    hypermodel,\n",
    "    max_epochs=HYPERBAND_MAX_EPOCHS,\n",
    "    objective= kt.Objective(\"val_auc\", direction=\"max\"),\n",
    "    seed=SEED,\n",
    "    hyperband_iterations=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vfbMmSc22mUz"
   },
   "outputs": [],
   "source": [
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NJWGHcX72mU2"
   },
   "outputs": [],
   "source": [
    "# Search best model\n",
    "tuner.search(trainset,\n",
    "             validation_data=valset,\n",
    "             epochs=30,\n",
    "             steps_per_epoch=resampled_steps_per_epoch,\n",
    "             callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4VmdGsRG2mVC"
   },
   "outputs": [],
   "source": [
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "USzFBqZB6h_V"
   },
   "source": [
    "Search for best deep neural network model structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7dPU138z_FSu"
   },
   "outputs": [],
   "source": [
    "from kerastuner import HyperModel\n",
    "\n",
    "\n",
    "class DNNHyperModel(HyperModel):\n",
    "    def __init__(self, input_shape):\n",
    "        self.input_shape = input_shape\n",
    "\n",
    "    def build(self, hp):\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(\n",
    "            tf.keras.layers.Dense(\n",
    "                hp.Int(\n",
    "                    'num_neuron_1',\n",
    "                    32,\n",
    "                    256,\n",
    "                    step=32),\n",
    "                activation='relu',\n",
    "                input_shape=self.input_shape\n",
    "            )\n",
    "        )\n",
    "        for i in range(hp.Int('Num_layers_', 2, 4, default=3)):\n",
    "          units = hp.Int('num_neuron_' + str(i), 32, 256, step=32)\n",
    "        \n",
    "          model.add(\n",
    "              tf.keras.layers.Dense(\n",
    "                  units,\n",
    "                  activation='relu'\n",
    "              )\n",
    "          )\n",
    "        model.add(\n",
    "              tf.keras.layers.Dropout(\n",
    "                  hp.Float('dropout', 0, 0.5, step=0.1, default=0.2)\n",
    "              )\n",
    "          )\n",
    "        model.add(\n",
    "            tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "        )\n",
    "        \n",
    "        model.compile(\n",
    "          optimizer=tf.keras.optimizers.Adam(lr=3e-4),\n",
    "          loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "          metrics=METRICS)\n",
    "        \n",
    "        return model\n",
    "\n",
    "INPUT_SHAPE = (data_train_point.shape[-1],)\n",
    "hypermodel = DNNHyperModel(input_shape=INPUT_SHAPE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9rbQyMtnCbfW"
   },
   "outputs": [],
   "source": [
    "\n",
    "HYPERBAND_MAX_EPOCHS = 40\n",
    "SEED=41\n",
    "tuner = kt.Hyperband(\n",
    "    hypermodel,\n",
    "    max_epochs=HYPERBAND_MAX_EPOCHS,\n",
    "    objective= kt.Objective(\"val_auc\", direction=\"max\"),\n",
    "    seed=SEED,\n",
    "    hyperband_iterations=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iiOh8cqc4Ak9"
   },
   "outputs": [],
   "source": [
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HYh9cihxFtnz"
   },
   "outputs": [],
   "source": [
    "# Search best model\n",
    "tuner.search(trainset,\n",
    "             validation_data=valset,\n",
    "             epochs=30,\n",
    "             steps_per_epoch=resampled_steps_per_epoch,\n",
    "             callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J_xV-R-KwF7S"
   },
   "outputs": [],
   "source": [
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9s9v5-8sLPr6"
   },
   "source": [
    "## Logistic Regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qc0YQ1sGy8j4"
   },
   "source": [
    "A Logistic regression with L2 normalization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h-_-D5CXHzGU"
   },
   "outputs": [],
   "source": [
    "# # without Time-of-Day variable\n",
    "def SimpleRegression(data_training,labels_training,data_holdout,labels_holdout, split_point=10):\n",
    "  \n",
    "  \n",
    "  aucs = []\n",
    "  recalls = []\n",
    "  precisions = []\n",
    "  auprcs = []\n",
    "  # remove time variable\n",
    "  data_training = data_training[:,:,:split_point]\n",
    "  data_holdout = data_holdout[:,:,:split_point]\n",
    "\n",
    "  # cross validation\n",
    "  for train_index, val_index in tqdm_notebook(KFold(n_splits=4, random_state=41, shuffle=True).split(data_training)):\n",
    "    data_train, data_val = data_training[train_index], data_training[val_index]\n",
    "    labels_train, labels_val = labels_training[train_index], labels_training[val_index]\n",
    "    # get overall count within 24 hrs\n",
    "    data_train_point = np.sum(data_train,axis=1)\n",
    "    data_val_point = np.sum(data_val,axis=1)\n",
    "    data_test_point = np.sum(data_holdout,axis=1)\n",
    "\n",
    "    valset = Create_dataset(data_val_point, labels_val)\n",
    "    testset = Create_dataset(data_test_point, labels_holdout)\n",
    "    resampled_ds = Resample_dataset(data_train_point,labels_train)\n",
    "    \n",
    "    regression = tf.keras.Sequential([\n",
    "          tf.keras.layers.Dense(1, kernel_regularizer=tf.keras.regularizers.l2(20), activation='sigmoid',\n",
    "              input_shape=(data_train_point.shape[-1],))\n",
    "                            \n",
    "      ])\n",
    "\n",
    "    regression.compile(\n",
    "          optimizer=tf.keras.optimizers.SGD(lr=1e-3),\n",
    "          loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "          metrics=METRICS)\n",
    "    #regression.summary()\n",
    "\n",
    "    # trainig\n",
    "    regression_history = regression.fit(\n",
    "      resampled_ds,\n",
    "      epochs=EPOCHS,\n",
    "      steps_per_epoch=resampled_steps_per_epoch,\n",
    "      callbacks = [early_stopping],\n",
    "      validation_data=valset,\n",
    "      verbose=0)\n",
    "\n",
    "    #validation\n",
    "    train_predictions = regression.predict(data_train_point, batch_size=BATCH_SIZE)\n",
    "    test_predictions = regression.predict(data_test_point, batch_size=BATCH_SIZE)\n",
    "  \n",
    "\n",
    "    _,_,auroc,precision,recall,_,_,_,_ = regression.evaluate(data_test_point, labels_holdout,\n",
    "                                             batch_size=BATCH_SIZE, verbose=0)\n",
    "  \n",
    "    aucs.append(auroc)\n",
    "    recalls.append(recall)\n",
    "    precisions.append(precision)\n",
    "    print(auroc, recall, precision)\n",
    "\n",
    "    precision, recall, thresholds = precision_recall_curve(labels_holdout, test_predictions)\n",
    "    auprc = auc(recall, precision)\n",
    "    auprcs.append(auprc)\n",
    "    print('PR AUC',auprc)\n",
    "    #plot_pr_curve('auprc',labels_holdout, test_predictions)\n",
    "    #plot_calibration('calibration',labels_holdout, test_predictions)\n",
    "  #plot_metrics(regression_history)  \n",
    "  auc_ave = np.array(aucs).mean()\n",
    "  recall_ave = np.array(recalls).mean()\n",
    "  precisions_ave = np.array(precisions).mean()\n",
    "  auprc_ave = np.array(auprcs).mean()\n",
    "  print(\"auc:{}, recall:{}, precision:{}\".format(auc_ave, recall_ave, precisions_ave))\n",
    "  print('AUPRC:{}'.format(auprc_ave))\n",
    "\n",
    "  \n",
    "  return test_predictions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MJ-v8-uWPN1c"
   },
   "outputs": [],
   "source": [
    "# with Time-of-Day variable\n",
    "def SimpleRegression_j(data_training,labels_training,data_holdout,labels_holdout, split_point=10):\n",
    "  \n",
    "  \n",
    "  aucs = []\n",
    "  recalls = []\n",
    "  precisions = []\n",
    "  auprcs = []\n",
    "  \n",
    "  # Separate Time-of-Day variable\n",
    "  julian_training = data_training[:,0,split_point:]\n",
    "  julina_holdout = data_holdout[:,0,split_point:]\n",
    "  data_training = data_training[:,:,:split_point]\n",
    "  data_holdout = data_holdout[:,:,:split_point]\n",
    "\n",
    "\n",
    "# cross validation\n",
    "  for train_index, val_index in tqdm_notebook(KFold(n_splits=4, random_state=41, shuffle=True).split(data_training)):\n",
    "    data_train, data_val = data_training[train_index], data_training[val_index]\n",
    "    labels_train, labels_val = labels_training[train_index], labels_training[val_index]\n",
    "    #julian\n",
    "    j_train, j_val = julian_training[train_index], julian_training[val_index]\n",
    "\n",
    "    # get overall count within 24hrs\n",
    "    data_train_point = np.sum(data_train,axis=1)\n",
    "    data_val_point = np.sum(data_val,axis=1)\n",
    "    data_test_point = np.sum(data_holdout,axis=1)\n",
    "    # concat dataset with Time-of-day variable\n",
    "    data_train_point = np.concatenate((data_train_point, j_train), axis=1) \n",
    "    data_val_point = np.concatenate((data_val_point, j_val), axis=1) \n",
    "    data_test_point = np.concatenate((data_test_point, julina_holdout), axis=1)\n",
    "    print(data_train_point.shape,data_val_point.shape,data_test_point.shape)\n",
    "\n",
    "    valset = Create_dataset(data_val_point, labels_val)\n",
    "    testset = Create_dataset(data_test_point, labels_holdout)\n",
    "    resampled_ds = Resample_dataset(data_train_point,labels_train)\n",
    "    #initialize model\n",
    "    regression = tf.keras.Sequential([\n",
    "          tf.keras.layers.Dense(1, kernel_regularizer=tf.keras.regularizers.l2(20), activation='sigmoid',\n",
    "              input_shape=(data_train_point.shape[-1],))\n",
    "                            \n",
    "      ])\n",
    "\n",
    "    regression.compile(\n",
    "          optimizer=tf.keras.optimizers.SGD(lr=1e-3),\n",
    "          loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "          metrics=METRICS)\n",
    "    #regression.summary()\n",
    "\n",
    "    # trainig\n",
    "    regression_history = regression.fit(\n",
    "      resampled_ds,\n",
    "      epochs=EPOCHS,\n",
    "      steps_per_epoch=resampled_steps_per_epoch,\n",
    "      callbacks = [early_stopping],\n",
    "      validation_data=valset,\n",
    "      verbose=0)\n",
    "\n",
    "    #validation\n",
    "    train_predictions = regression.predict(data_train_point, batch_size=BATCH_SIZE)\n",
    "    test_predictions = regression.predict(data_test_point, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    _,_,auroc,precision,recall,_,_,_,_ = regression.evaluate(data_test_point, labels_holdout,\n",
    "                                             batch_size=BATCH_SIZE, verbose=0)\n",
    "    \n",
    "    aucs.append(auroc)\n",
    "    recalls.append(recall)\n",
    "    precisions.append(precision)\n",
    "    print(auroc, recall, precision)\n",
    "\n",
    "    precision, recall, thresholds = precision_recall_curve(labels_holdout, test_predictions)\n",
    "    auprc = auc(recall, precision)\n",
    "    auprcs.append(auprc)\n",
    "    print('PR AUC',auprc)\n",
    "  #plot_metrics(regression_history)  \n",
    "  auc_ave = np.array(aucs).mean()\n",
    "  recall_ave = np.array(recalls).mean()\n",
    "  precisions_ave = np.array(precisions).mean()\n",
    "  auprc_ave = np.array(auprcs).mean()\n",
    "  print(\"auc:{}, recall:{}, precision:{}\".format(auc_ave, recall_ave, precisions_ave))\n",
    "  print('AUPRC:{}'.format(auprc_ave))\n",
    "  return test_predictions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q1SvGCSLLf_h"
   },
   "source": [
    "## Deep Neural Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WE4OVNDg05J7"
   },
   "source": [
    "This deep neural network structure is defined by the result of KerasTuner. A three-layered deep neural network with a drop-out layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YLbmnq9JSV7P"
   },
   "outputs": [],
   "source": [
    "# # without Time-of-Day variable\n",
    "def DNN(data_training,labels_training,data_holdout,labels_holdout,split_point=10):\n",
    "\n",
    "  \n",
    "  aucs = []\n",
    "  recalls = []\n",
    "  precisions = []\n",
    "  auprcs=[]\n",
    "  \n",
    "  #remove Time-of-day variable\n",
    "  data_training = data_training[:,:,:split_point]\n",
    "  data_holdout = data_holdout[:,:,:split_point]\n",
    "\n",
    "\n",
    "# cross validation\n",
    "  for train_index, val_index in tqdm_notebook(KFold(n_splits=4, random_state=41, shuffle=True).split(data_training)):\n",
    "    data_train, data_val = data_training[train_index], data_training[val_index]\n",
    "    labels_train, labels_val = labels_training[train_index], labels_training[val_index]\n",
    "    data_train_point = np.sum(data_train,axis=1)\n",
    "    data_val_point = np.sum(data_val,axis=1)\n",
    "    data_test_point = np.sum(data_holdout,axis=1)\n",
    "\n",
    "    valset = Create_dataset(data_val_point, labels_val)\n",
    "    testset = Create_dataset(data_test_point, labels_holdout)\n",
    "    resampled_ds = Resample_dataset(data_train_point,labels_train)\n",
    "     \n",
    "    # Initialize model\n",
    "    FFN = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(\n",
    "            128, activation='relu',\n",
    "            input_shape=(data_train_point.shape[-1],)),\n",
    "        tf.keras.layers.Dense(\n",
    "            32, activation='relu'),\n",
    "        tf.keras.layers.Dense(\n",
    "            160, activation='relu'),\n",
    "        \n",
    "        tf.keras.layers.Dropout(0.1),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "                          \n",
    "    ])\n",
    "\n",
    "    FFN.compile(\n",
    "          optimizer=tf.keras.optimizers.SGD(lr=1e-3),\n",
    "          loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "          metrics=METRICS)\n",
    "    #regression.summary()\n",
    "\n",
    "    # trainig\n",
    "    history = FFN.fit(\n",
    "      resampled_ds,\n",
    "      epochs=EPOCHS,\n",
    "      steps_per_epoch=resampled_steps_per_epoch,\n",
    "      callbacks = [early_stopping],\n",
    "      validation_data=valset,\n",
    "      verbose=0)\n",
    "\n",
    "    #validation\n",
    "    train_predictions = FFN.predict(data_train_point, batch_size=BATCH_SIZE)\n",
    "    test_predictions = FFN.predict(data_test_point, batch_size=BATCH_SIZE)\n",
    "\n",
    "    _,_,auroc,precision,recall,_,_,_,_ = FFN.evaluate(data_test_point, labels_holdout,\n",
    "                                             batch_size=BATCH_SIZE, verbose=0)\n",
    "    aucs.append(auroc)\n",
    "    recalls.append(recall)\n",
    "    precisions.append(precision)\n",
    "    print(auroc, recall, precision)\n",
    "\n",
    "    precision, recall, thresholds = precision_recall_curve(labels_holdout, test_predictions)\n",
    "    auprc = auc(recall, precision)\n",
    "    auprcs.append(auprc)\n",
    "    print('PR AUC',auprc)\n",
    "  #plot_metrics(regression_history)  \n",
    "  auc_ave = np.array(aucs).mean()\n",
    "  recall_ave = np.array(recalls).mean()\n",
    "  precisions_ave = np.array(precisions).mean()\n",
    "  auprc_ave = np.array(auprcs).mean()\n",
    "  print(\"auc:{}, recall:{}, precision:{}\".format(auc_ave, recall_ave, precisions_ave))\n",
    "  print('AUPRC:{}'.format(auprc_ave))\n",
    "  return test_predictions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uONGQ0gaGuf8"
   },
   "outputs": [],
   "source": [
    "# # with Time-of-Day variable\n",
    "def DNN_j(data_training,labels_training,data_holdout,labels_holdout,split_point=10):\n",
    "\n",
    "  \n",
    "  aucs = []\n",
    "  recalls = []\n",
    "  precisions = []\n",
    "  auprcs = []\n",
    "  #Separate Time-of_day variable\n",
    "  julian_training = data_training[:,0,split_point:]\n",
    "  julina_holdout = data_holdout[:,0,split_point:]\n",
    "  data_training = data_training[:,:,:split_point]\n",
    "  data_holdout = data_holdout[:,:,:split_point]\n",
    "\n",
    "\n",
    "# cross validation\n",
    "  for train_index, val_index in tqdm_notebook(KFold(n_splits=4, random_state=41, shuffle=True).split(data_training)):\n",
    "    data_train, data_val = data_training[train_index], data_training[val_index]\n",
    "    labels_train, labels_val = labels_training[train_index], labels_training[val_index]\n",
    "    j_train, j_val = julian_training[train_index], julian_training[val_index]\n",
    "\n",
    "    # get overall count within 24hrs\n",
    "    data_train_point = np.sum(data_train,axis=1)\n",
    "    data_val_point = np.sum(data_val,axis=1)\n",
    "    data_test_point = np.sum(data_holdout,axis=1)\n",
    "    # concat dataset with Time-of-day variable\n",
    "    data_train_point = np.concatenate((data_train_point, j_train), axis=1) \n",
    "    data_val_point = np.concatenate((data_val_point, j_val), axis=1) \n",
    "    data_test_point = np.concatenate((data_test_point, julina_holdout), axis=1)\n",
    "    print(data_train_point.shape,data_val_point.shape,data_test_point.shape)\n",
    "\n",
    "    valset = Create_dataset(data_val_point, labels_val)\n",
    "    testset = Create_dataset(data_test_point, labels_holdout)\n",
    "    resampled_ds = Resample_dataset(data_train_point,labels_train)\n",
    "    \n",
    "    # Initialize model\n",
    "    FFN = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(\n",
    "            128, activation='relu',\n",
    "            input_shape=(data_train_point.shape[-1],)),\n",
    "        tf.keras.layers.Dense(\n",
    "            32, activation='relu'),\n",
    "        tf.keras.layers.Dense(\n",
    "            160, activation='relu'),\n",
    "        \n",
    "        tf.keras.layers.Dropout(0.1),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "                          \n",
    "    ])\n",
    "\n",
    "    FFN.compile(\n",
    "          optimizer=tf.keras.optimizers.Adam(lr=3e-4),\n",
    "          loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "          metrics=METRICS)\n",
    "    #regression.summary()\n",
    "\n",
    "    # trainig\n",
    "    history = FFN.fit(\n",
    "      resampled_ds,\n",
    "      epochs=EPOCHS,\n",
    "      steps_per_epoch=resampled_steps_per_epoch,\n",
    "      callbacks = [early_stopping],\n",
    "      validation_data=valset,\n",
    "      verbose=0)\n",
    "\n",
    "    #validation\n",
    "    train_predictions = FFN.predict(data_train_point, batch_size=BATCH_SIZE)\n",
    "    test_predictions = FFN.predict(data_test_point, batch_size=BATCH_SIZE)\n",
    "\n",
    "    _,_,auroc,precision,recall,_,_,_,_ = FFN.evaluate(data_test_point, labels_holdout,\n",
    "                                             batch_size=BATCH_SIZE, verbose=0)\n",
    "    aucs.append(auroc)\n",
    "    recalls.append(recall)\n",
    "    precisions.append(precision)\n",
    "    print(auroc, recall, precision)\n",
    "\n",
    "    precision, recall, thresholds = precision_recall_curve(labels_holdout, test_predictions)\n",
    "    auprc = auc(recall, precision)\n",
    "    auprcs.append(auprc)\n",
    "    print('PR AUC',auprc)\n",
    "  #plot_metrics(regression_history)  \n",
    "  auc_ave = np.array(aucs).mean()\n",
    "  recall_ave = np.array(recalls).mean()\n",
    "  precisions_ave = np.array(precisions).mean()\n",
    "  auprc_ave = np.array(auprcs).mean()\n",
    "  print(\"auc:{}, recall:{}, precision:{}\".format(auc_ave, recall_ave, precisions_ave))\n",
    "  print('AUPRC:{}'.format(auprc_ave))\n",
    "\n",
    "  return test_predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C8n5GpPYLldT"
   },
   "source": [
    "## Long Short Term Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VK4MhUb-6_2f"
   },
   "source": [
    "Single layer LSTM with one dropout layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UlXJDxi5gBFp"
   },
   "outputs": [],
   "source": [
    "def LSTM(data_training,labels_training,data_holdout,labels_holdout, jdummies= False, split_point=10):\n",
    "  # Single layer LSTM with dropout layer\n",
    "  \n",
    "  aucs = []\n",
    "  recalls = []\n",
    "  precisions = []\n",
    "  auprcs = []\n",
    "\n",
    "  if jdummies:\n",
    "    pass\n",
    "  else: \n",
    "    data_training = data_training[:,:,:split_point]\n",
    "    data_holdout = data_holdout[:,:,:split_point]\n",
    "\n",
    "\n",
    "# cross validation\n",
    "  for train_index, val_index in tqdm_notebook(KFold(n_splits=4, random_state=41, shuffle=True).split(data_training)):\n",
    "    data_train, data_val = data_training[train_index], data_training[val_index]\n",
    "    labels_train, labels_val = labels_training[train_index], labels_training[val_index]\n",
    "\n",
    "    valset = Create_dataset(data_val, labels_val)\n",
    "    #testset = Create_dataset(data_holdout, labels_holdout)\n",
    "    resampled_ds = Resample_dataset(data_train,labels_train)\n",
    "    \n",
    "    # Initiate model\n",
    "    LSTM = tf.keras.Sequential([\n",
    "          tf.keras.layers.LSTM(32,\n",
    "              input_shape=data_train.shape[-2:]),\n",
    "               tf.keras.layers.Dropout(0.3),\n",
    "          tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "                            \n",
    "      ])\n",
    "\n",
    "    LSTM.compile(\n",
    "          optimizer=tf.keras.optimizers.Adam(lr=3e-4),\n",
    "          loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "          metrics=METRICS)\n",
    "    #regression.summary()\n",
    "\n",
    "    # trainig\n",
    "    history = LSTM.fit(\n",
    "      resampled_ds,\n",
    "      epochs=EPOCHS,\n",
    "      steps_per_epoch=resampled_steps_per_epoch,\n",
    "      callbacks = [early_stopping],\n",
    "      validation_data=valset,\n",
    "      verbose=0)\n",
    "\n",
    "    #validation\n",
    "    #train_predictions = LSTM.predict(data_train, batch_size=BATCH_SIZE)\n",
    "    test_predictions = LSTM.predict(data_holdout, batch_size=BATCH_SIZE)\n",
    "\n",
    "    _,_,auroc,precision,recall,_,_,_,_ = LSTM.evaluate(data_holdout, labels_holdout,\n",
    "                                             batch_size=BATCH_SIZE, verbose=0)\n",
    "    aucs.append(auroc)\n",
    "    recalls.append(recall)\n",
    "    precisions.append(precision)\n",
    "    print(auroc, recall, precision)\n",
    "\n",
    "    precision, recall, thresholds = precision_recall_curve(labels_holdout, test_predictions)\n",
    "    auprc = auc(recall, precision)\n",
    "    auprcs.append(auprc)\n",
    "    print('PR AUC',auprc)\n",
    "  #plot_metrics(regression_history)  \n",
    "  auc_ave = np.array(aucs).mean()\n",
    "  recall_ave = np.array(recalls).mean()\n",
    "  precisions_ave = np.array(precisions).mean()\n",
    "  auprc_ave = np.array(auprcs).mean()\n",
    "  print(\"auc:{}, recall:{}, precision:{}\".format(auc_ave, recall_ave, precisions_ave))\n",
    "  print('AUPRC:{}'.format(auprc_ave))\n",
    "  return test_predictions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3k4Ck-LvLoRA"
   },
   "source": [
    "## Gated Recurrent Unit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VEk_qtpZ7FHV"
   },
   "source": [
    "Single layer GRU with one dropout layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nW6ym_PDi9sR"
   },
   "outputs": [],
   "source": [
    "def GRU(data_training,labels_training,data_holdout,labels_holdout, jdummies= False, split_point=10):\n",
    "\n",
    "  aucs = []\n",
    "  recalls = []\n",
    "  precisions = []\n",
    "  auprcs=[]\n",
    "\n",
    "  if jdummies:\n",
    "    pass\n",
    "  else: \n",
    "    data_training = data_training[:,:,:split_point]\n",
    "    data_holdout = data_holdout[:,:,:split_point]\n",
    "\n",
    "\n",
    "# cross validation\n",
    "  for train_index, val_index in tqdm_notebook(KFold(n_splits=4, random_state=41, shuffle=True).split(data_training)):\n",
    "    data_train, data_val = data_training[train_index], data_training[val_index]\n",
    "    labels_train, labels_val = labels_training[train_index], labels_training[val_index]\n",
    "\n",
    "    valset = Create_dataset(data_val, labels_val)\n",
    "    #testset = Create_dataset(data_holdout, labels_holdout)\n",
    "    resampled_ds = Resample_dataset(data_train,labels_train)\n",
    "     \n",
    "    # Initiate model\n",
    "    GRU = tf.keras.Sequential([\n",
    "          tf.keras.layers.GRU(32,\n",
    "              input_shape=data_train.shape[-2:]),\n",
    "              tf.keras.layers.Dropout(0.3),\n",
    "          tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "                            \n",
    "      ])\n",
    "\n",
    "    GRU.compile(\n",
    "          optimizer=tf.keras.optimizers.Adam(lr=3e-4),\n",
    "          loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "          metrics=METRICS)\n",
    "    #regression.summary()\n",
    "\n",
    "    # trainig\n",
    "    history = GRU.fit(\n",
    "      resampled_ds,\n",
    "      epochs=EPOCHS,\n",
    "      steps_per_epoch=resampled_steps_per_epoch,\n",
    "      callbacks = [early_stopping],\n",
    "      validation_data=valset,\n",
    "      verbose=0)\n",
    "\n",
    "    #validation\n",
    "    #train_predictions = GRU.predict(data_train, batch_size=BATCH_SIZE)\n",
    "    test_predictions = GRU.predict(data_holdout, batch_size=BATCH_SIZE)\n",
    "\n",
    "    _,_,auroc,precision,recall,_,_,_,_ = GRU.evaluate(data_holdout, labels_holdout,\n",
    "                                             batch_size=BATCH_SIZE, verbose=0)\n",
    "    aucs.append(auroc)\n",
    "    recalls.append(recall)\n",
    "    precisions.append(precision)\n",
    "    print(auroc, recall, precision)\n",
    "\n",
    "    precision, recall, thresholds = precision_recall_curve(labels_holdout, test_predictions)\n",
    "    auprc = auc(recall, precision)\n",
    "    auprcs.append(auprc)\n",
    "    print('PR AUC',auprc)\n",
    "  #plot_metrics(regression_history)  \n",
    "  auc_ave = np.array(aucs).mean()\n",
    "  recall_ave = np.array(recalls).mean()\n",
    "  precisions_ave = np.array(precisions).mean()\n",
    "  auprc_ave = np.array(auprcs).mean()\n",
    "  print(\"auc:{}, recall:{}, precision:{}\".format(auc_ave, recall_ave, precisions_ave))\n",
    "  print('AUPRC:{}'.format(auprc_ave))\n",
    "  return test_predictions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BwAyH-1K7bIR"
   },
   "source": [
    "#Model Development and Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MTj1h9KTiavU"
   },
   "source": [
    "## 15 minute time block with 5 features\n",
    "* vital sign measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "14Dg0E-CiavW"
   },
   "outputs": [],
   "source": [
    "tdata_name,tlabels_name,hdata_name,hlabels_name = \"tseq_15Tr5.npy\", 'tlabels_15Tr5.npy', \"hseq_15Tr5.npy\", 'hlabels_15Tr5.npy'\n",
    "d = 'last3612/'\n",
    "my_data_dir = 'Colab Notebooks/'+d\n",
    "data_training = np.load(\"gdrive/My Drive/\"+my_data_dir + tdata_name)\n",
    "labels_training = np.load(\"gdrive/My Drive/\"+my_data_dir + tlabels_name)\n",
    "\n",
    "data_holdout = np.load(\"gdrive/My Drive/\"+my_data_dir + hdata_name)\n",
    "labels_holdout = np.load(\"gdrive/My Drive/\"+my_data_dir + hlabels_name)\n",
    "\n",
    "#view data\n",
    "print(labels_training.mean(), labels_holdout.mean())\n",
    "print(data_training.shape, data_holdout.shape)\n",
    "\n",
    "#\n",
    "pos_data = data_training[labels_training != 0]\n",
    "neg_data = data_training[labels_training == 0]\n",
    "pos_data= np.sum(pos_data,axis=1)\n",
    "neg_data= np.sum(neg_data,axis=1)\n",
    "print(pd.DataFrame({'outcome':pos_data.mean(axis=0),'survival':neg_data.mean(axis=0)}))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RrxejE2Iiava"
   },
   "outputs": [],
   "source": [
    "SimpleRegression(data_training,labels_training,data_holdout,labels_holdout,split_point=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OE40w3TWiavc"
   },
   "outputs": [],
   "source": [
    "DNN(data_training,labels_training,data_holdout,labels_holdout, split_point=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5jGSPwINiavf"
   },
   "outputs": [],
   "source": [
    "LSTM(data_training,labels_training,data_holdout,labels_holdout, jdummies= False, split_point=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "boCV4IeJiavh"
   },
   "outputs": [],
   "source": [
    "GRU(data_training,labels_training,data_holdout,labels_holdout, jdummies= False, split_point=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0HawtC-kBAns"
   },
   "outputs": [],
   "source": [
    "SimpleRegression_j(data_training,labels_training,data_holdout,labels_holdout,split_point=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qy1a0DfsBAnx"
   },
   "outputs": [],
   "source": [
    "DNN_j(data_training,labels_training,data_holdout,labels_holdout, split_point=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vq8E1J1riavj"
   },
   "outputs": [],
   "source": [
    "LSTM(data_training,labels_training,data_holdout,labels_holdout, jdummies= True, split_point=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4IeFQ0BWiavo"
   },
   "outputs": [],
   "source": [
    "GRU(data_training,labels_training,data_holdout,labels_holdout, jdummies= True, split_point=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OGvJQ-iTjmSm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tWjX1x8sE8oL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-V1-ArhXmHeX"
   },
   "source": [
    "## 15 minute time block with 9 features\n",
    "† vital sign measurements, flowsheets comments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wAtNef41UiFd"
   },
   "outputs": [],
   "source": [
    "tdata_name,tlabels_name,hdata_name,hlabels_name = \"tseq_15Tr10jd.npy\", 'tlabels_15Tr10jd.npy', \"hseq_15Tr10jd.npy\", 'hlabels_15Tr10jd.npy'\n",
    "\n",
    "data_training = np.load(\"gdrive/My Drive/\"+my_data_dir + tdata_name)\n",
    "labels_training = np.load(\"gdrive/My Drive/\"+my_data_dir + tlabels_name)\n",
    "\n",
    "data_holdout = np.load(\"gdrive/My Drive/\"+my_data_dir + hdata_name)\n",
    "labels_holdout = np.load(\"gdrive/My Drive/\"+my_data_dir + hlabels_name)\n",
    "\n",
    "#view data\n",
    "print(labels_training.mean(), labels_holdout.mean())\n",
    "print(data_training.shape, data_holdout.shape)\n",
    "\n",
    "#\n",
    "pos_data = data_training[labels_training != 0]\n",
    "neg_data = data_training[labels_training == 0]\n",
    "pos_data= np.sum(pos_data,axis=1)\n",
    "neg_data= np.sum(neg_data,axis=1)\n",
    "print(pd.DataFrame({'outcome':pos_data.mean(axis=0),'survival':neg_data.mean(axis=0)}))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "otJQwBRgmQmv"
   },
   "outputs": [],
   "source": [
    "SimpleRegression(data_training,labels_training,data_holdout,labels_holdout,split_point=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2l5TKrCgmQrF"
   },
   "outputs": [],
   "source": [
    "DNN(data_training,labels_training,data_holdout,labels_holdout, split_point=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5_netGU7mQNh"
   },
   "outputs": [],
   "source": [
    "LSTM(data_training,labels_training,data_holdout,labels_holdout, jdummies= False, split_point=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fP3ARdmFmkKj"
   },
   "outputs": [],
   "source": [
    "GRU(data_training,labels_training,data_holdout,labels_holdout, jdummies= False, split_point=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HkSW0_oqAUEo"
   },
   "outputs": [],
   "source": [
    "reg_pred = SimpleRegression_j(data_training,labels_training,data_holdout,labels_holdout,split_point=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ii7CZ_YqAUEv"
   },
   "outputs": [],
   "source": [
    "dnn_pred = DNN_j(data_training,labels_training,data_holdout,labels_holdout, split_point=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mVvsAEcCmqvE"
   },
   "outputs": [],
   "source": [
    "LSTM(data_training,labels_training,data_holdout,labels_holdout, jdummies= True, split_point=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8fDVdLeAmRCS"
   },
   "outputs": [],
   "source": [
    "GRU(data_training,labels_training,data_holdout,labels_holdout, jdummies= True, split_point=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JHnMmGzInQ8m"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hwSavsSGnZxU"
   },
   "source": [
    "## 15 minutes time block with 15 features\n",
    "‡ vital sign measurements, flowsheets comments, order entry, nursing notes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dNFlVnOSnmrW"
   },
   "outputs": [],
   "source": [
    "tdata_name,tlabels_name,hdata_name,hlabels_name = \"tseq_15Tr15jd.npy\", 'tlabels_15Tr15jd.npy', \"hseq_15Tr15jd.npy\", 'hlabels_15Tr15jd.npy'\n",
    "data_training = np.load(\"gdrive/My Drive/\"+my_data_dir + tdata_name)\n",
    "labels_training = np.load(\"gdrive/My Drive/\"+my_data_dir + tlabels_name)\n",
    "\n",
    "data_holdout = np.load(\"gdrive/My Drive/\"+my_data_dir + hdata_name)\n",
    "labels_holdout = np.load(\"gdrive/My Drive/\"+my_data_dir + hlabels_name)\n",
    "\n",
    "#view data\n",
    "print(labels_training.mean(), labels_holdout.mean())\n",
    "print(data_training.shape, data_holdout.shape)\n",
    "\n",
    "#\n",
    "pos_data = data_training[labels_training != 0]\n",
    "neg_data = data_training[labels_training == 0]\n",
    "pos_data= np.sum(pos_data,axis=1)\n",
    "neg_data= np.sum(neg_data,axis=1)\n",
    "print(pd.DataFrame({'outcome':pos_data.mean(axis=0),'survival':neg_data.mean(axis=0)}))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8heipEZ1nmrl"
   },
   "outputs": [],
   "source": [
    "SimpleRegression(data_training,labels_training,data_holdout,labels_holdout, split_point=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oJVuB5dinmrp"
   },
   "outputs": [],
   "source": [
    "DNN(data_training,labels_training,data_holdout,labels_holdout, split_point=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kr0JuJ4Cnmrw"
   },
   "outputs": [],
   "source": [
    "LSTM(data_training,labels_training,data_holdout,labels_holdout, jdummies= False, split_point=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UPJjNrkKnmr0"
   },
   "outputs": [],
   "source": [
    "GRU(data_training,labels_training,data_holdout,labels_holdout, jdummies= False, split_point=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "llfNRYiCAYUy"
   },
   "outputs": [],
   "source": [
    "SimpleRegression_j(data_training,labels_training,data_holdout,labels_holdout,split_point=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tzG2DhuHAYU3"
   },
   "outputs": [],
   "source": [
    "DNN_j(data_training,labels_training,data_holdout,labels_holdout, split_point=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ywX8cMRonmr3"
   },
   "outputs": [],
   "source": [
    "LSTM(data_training,labels_training,data_holdout,labels_holdout, jdummies= True, split_point=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h-wVx_4dnmr5"
   },
   "outputs": [],
   "source": [
    "GRU(data_training,labels_training,data_holdout,labels_holdout, jdummies= True, split_point=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9buvCifyng26"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4YQb0ezoopU4"
   },
   "source": [
    "## 15 minute time block with 62 features\n",
    "$ vital sign measurements, flowsheets comments, order entry, nursing notes, occurrence of entities extracted from nursing notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BzoY13E0pZB_"
   },
   "outputs": [],
   "source": [
    "tdata_name,tlabels_name,hdata_name,hlabels_name = \"tseq_15Tr_all.npy\", 'tlabels_15Tr_all.npy', \"hseq_15Tr_all.npy\", 'hlabels_15Tr_all.npy'\n",
    "data_training = np.load(\"gdrive/My Drive/\"+my_data_dir + tdata_name)\n",
    "labels_training = np.load(\"gdrive/My Drive/\"+my_data_dir + tlabels_name)\n",
    "\n",
    "data_holdout = np.load(\"gdrive/My Drive/\"+my_data_dir + hdata_name)\n",
    "labels_holdout = np.load(\"gdrive/My Drive/\"+my_data_dir + hlabels_name)\n",
    "\n",
    "#view data\n",
    "print(labels_training.mean(), labels_holdout.mean())\n",
    "print(data_training.shape, data_holdout.shape)\n",
    "\n",
    "#\n",
    "pos_data = data_training[labels_training != 0]\n",
    "neg_data = data_training[labels_training == 0]\n",
    "pos_data= np.sum(pos_data,axis=1)\n",
    "neg_data= np.sum(neg_data,axis=1)\n",
    "print(pd.DataFrame({'outcome':pos_data.mean(axis=0),'survival':neg_data.mean(axis=0)}))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Voe34MTapZCK"
   },
   "outputs": [],
   "source": [
    "SimpleRegression(data_training,labels_training,data_holdout,labels_holdout, split_point=62)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qcJSTDLGpZCP"
   },
   "outputs": [],
   "source": [
    "DNN(data_training,labels_training,data_holdout,labels_holdout, split_point=62)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uecjW1vVpZCU"
   },
   "outputs": [],
   "source": [
    "LSTM(data_training,labels_training,data_holdout,labels_holdout, jdummies= False, split_point=62)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9zc8GdkwpZCX"
   },
   "outputs": [],
   "source": [
    "GRU(data_training,labels_training,data_holdout,labels_holdout, jdummies= False, split_point=62)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QnAdX2EiAcaV"
   },
   "outputs": [],
   "source": [
    "SimpleRegression_j(data_training,labels_training,data_holdout,labels_holdout,split_point=62)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "quKrjjQKAcab"
   },
   "outputs": [],
   "source": [
    "DNN_j(data_training,labels_training,data_holdout,labels_holdout, split_point=62)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eWr2vQYspZCa"
   },
   "outputs": [],
   "source": [
    "LSTM(data_training,labels_training,data_holdout,labels_holdout, jdummies= True, split_point=62)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yhgWiTDxpZCc"
   },
   "outputs": [],
   "source": [
    "GRU(data_training,labels_training,data_holdout,labels_holdout, jdummies= True, split_point=62)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F7kK-ZBYqgc3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gzbeBkQXjm4L"
   },
   "source": [
    "## 30 minute time block with 5 features\n",
    "* vital sign measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PXjbq-qNjm4M"
   },
   "outputs": [],
   "source": [
    "tdata_name,tlabels_name,hdata_name,hlabels_name = \"tseq_30Tr5.npy\", 'tlabels_30Tr5.npy', \"hseq_30Tr5.npy\", 'hlabels_30Tr5.npy'\n",
    "\n",
    "data_training = np.load(\"gdrive/My Drive/\"+my_data_dir + tdata_name)\n",
    "labels_training = np.load(\"gdrive/My Drive/\"+my_data_dir + tlabels_name)\n",
    "\n",
    "data_holdout = np.load(\"gdrive/My Drive/\"+my_data_dir + hdata_name)\n",
    "labels_holdout = np.load(\"gdrive/My Drive/\"+my_data_dir + hlabels_name)\n",
    "\n",
    "#view data\n",
    "print(labels_training.mean(), labels_holdout.mean())\n",
    "print(data_training.shape, data_holdout.shape)\n",
    "\n",
    "#\n",
    "pos_data = data_training[labels_training != 0]\n",
    "neg_data = data_training[labels_training == 0]\n",
    "pos_data= np.sum(pos_data,axis=1)\n",
    "neg_data= np.sum(neg_data,axis=1)\n",
    "print(pd.DataFrame({'outcome':pos_data.mean(axis=0),'survival':neg_data.mean(axis=0)}))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GbnkppMvjm4P"
   },
   "outputs": [],
   "source": [
    "SimpleRegression(data_training,labels_training,data_holdout,labels_holdout,split_point=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5vjSrJ82jm4T"
   },
   "outputs": [],
   "source": [
    "DNN(data_training,labels_training,data_holdout,labels_holdout, split_point=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GsxTalLPjm4a"
   },
   "outputs": [],
   "source": [
    "LSTM(data_training,labels_training,data_holdout,labels_holdout, jdummies= False, split_point=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nskaMrhjjm4e"
   },
   "outputs": [],
   "source": [
    "GRU(data_training,labels_training,data_holdout,labels_holdout, jdummies= False, split_point=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x0y71tIYBEUo"
   },
   "outputs": [],
   "source": [
    "SimpleRegression_j(data_training,labels_training,data_holdout,labels_holdout,split_point=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9006YkdrBEUu"
   },
   "outputs": [],
   "source": [
    "DNN_j(data_training,labels_training,data_holdout,labels_holdout, split_point=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gfmuofthjm4q"
   },
   "outputs": [],
   "source": [
    "LSTM(data_training,labels_training,data_holdout,labels_holdout, jdummies= True, split_point=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L7gUBLyojm4x"
   },
   "outputs": [],
   "source": [
    "GRU(data_training,labels_training,data_holdout,labels_holdout, jdummies= True, split_point=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2QVmOKk1juF3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nzCdyz4s3Ujo"
   },
   "source": [
    "## 30 minute time block with 9 features\n",
    "† vital sign measurements, flowsheets comments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ownVcDcD3Ujy"
   },
   "outputs": [],
   "source": [
    "tdata_name,tlabels_name,hdata_name,hlabels_name = \"tseq_30Tr10jd.npy\", 'tlabels_30Tr10jd.npy', \"hseq_30Tr10jd.npy\", 'hlabels_30Tr10jd.npy'\n",
    "\n",
    "data_training = np.load(\"gdrive/My Drive/\"+my_data_dir + tdata_name)\n",
    "labels_training = np.load(\"gdrive/My Drive/\"+my_data_dir + tlabels_name)\n",
    "\n",
    "data_holdout = np.load(\"gdrive/My Drive/\"+my_data_dir + hdata_name)\n",
    "labels_holdout = np.load(\"gdrive/My Drive/\"+my_data_dir + hlabels_name)\n",
    "\n",
    "#view data\n",
    "print(labels_training.mean(), labels_holdout.mean())\n",
    "print(data_training.shape, data_holdout.shape)\n",
    "\n",
    "#\n",
    "pos_data = data_training[labels_training != 0]\n",
    "neg_data = data_training[labels_training == 0]\n",
    "pos_data= np.sum(pos_data,axis=1)\n",
    "neg_data= np.sum(neg_data,axis=1)\n",
    "print(pd.DataFrame({'outcome':pos_data.mean(axis=0),'survival':neg_data.mean(axis=0)}))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X4X_dtHm3Uj4"
   },
   "outputs": [],
   "source": [
    "SimpleRegression(data_training,labels_training,data_holdout,labels_holdout,split_point=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gw2hqM6G3Uj9"
   },
   "outputs": [],
   "source": [
    "DNN(data_training,labels_training,data_holdout,labels_holdout, split_point=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cciWRrtT3UkB"
   },
   "outputs": [],
   "source": [
    "LSTM(data_training,labels_training,data_holdout,labels_holdout, jdummies= False, split_point=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cf9kFffz3UkE"
   },
   "outputs": [],
   "source": [
    "GRU(data_training,labels_training,data_holdout,labels_holdout, jdummies= False, split_point=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3tsIk27NAqwM"
   },
   "outputs": [],
   "source": [
    "SimpleRegression_j(data_training,labels_training,data_holdout,labels_holdout,split_point=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2tG8mawkAqwR"
   },
   "outputs": [],
   "source": [
    "DNN_j(data_training,labels_training,data_holdout,labels_holdout, split_point=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "84KARrIv3UkH"
   },
   "outputs": [],
   "source": [
    "LSTM(data_training,labels_training,data_holdout,labels_holdout, jdummies= True, split_point=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u6xZ8Xtm3UkJ"
   },
   "outputs": [],
   "source": [
    "GRU(data_training,labels_training,data_holdout,labels_holdout, jdummies= True, split_point=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N9dFMtW13goa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jQjtR2TV3wi2"
   },
   "source": [
    "## 30 minutes time block with 15 features\n",
    "‡ vital sign measurements, flowsheets comments, order entry, nursing notes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZCd7gOyf3wi4"
   },
   "outputs": [],
   "source": [
    "tdata_name,tlabels_name,hdata_name,hlabels_name = \"tseq_30Tr15jd.npy\", 'tlabels_30Tr15jd.npy', \"hseq_30Tr15jd.npy\", 'hlabels_30Tr15jd.npy'\n",
    "\n",
    "data_training = np.load(\"gdrive/My Drive/\"+my_data_dir + tdata_name)\n",
    "labels_training = np.load(\"gdrive/My Drive/\"+my_data_dir + tlabels_name)\n",
    "\n",
    "data_holdout = np.load(\"gdrive/My Drive/\"+my_data_dir + hdata_name)\n",
    "labels_holdout = np.load(\"gdrive/My Drive/\"+my_data_dir + hlabels_name)\n",
    "\n",
    "#view data\n",
    "print(labels_training.mean(), labels_holdout.mean())\n",
    "print(data_training.shape, data_holdout.shape)\n",
    "\n",
    "#\n",
    "pos_data = data_training[labels_training != 0]\n",
    "neg_data = data_training[labels_training == 0]\n",
    "pos_data= np.sum(pos_data,axis=1)\n",
    "neg_data= np.sum(neg_data,axis=1)\n",
    "print(pd.DataFrame({'outcome':pos_data.mean(axis=0),'survival':neg_data.mean(axis=0)}))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C30GQPPN3wi7"
   },
   "outputs": [],
   "source": [
    "SimpleRegression(data_training,labels_training,data_holdout,labels_holdout, split_point=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j-wZz8Xc3wi-"
   },
   "outputs": [],
   "source": [
    "DNN(data_training,labels_training,data_holdout,labels_holdout, split_point=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rDP69F1s3wjB"
   },
   "outputs": [],
   "source": [
    "LSTM(data_training,labels_training,data_holdout,labels_holdout, jdummies= False, split_point=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bUmzsTIw3wjE"
   },
   "outputs": [],
   "source": [
    "GRU(data_training,labels_training,data_holdout,labels_holdout, jdummies= False, split_point=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rsy5-2cGAvKp"
   },
   "outputs": [],
   "source": [
    "SimpleRegression_j(data_training,labels_training,data_holdout,labels_holdout,split_point=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SK1CIxwHAvKw"
   },
   "outputs": [],
   "source": [
    "DNN_j(data_training,labels_training,data_holdout,labels_holdout, split_point=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G1PObAzZ3wjH"
   },
   "outputs": [],
   "source": [
    "LSTM(data_training,labels_training,data_holdout,labels_holdout, jdummies= True, split_point=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MFdyRWiw3wjJ"
   },
   "outputs": [],
   "source": [
    "GRU(data_training,labels_training,data_holdout,labels_holdout, jdummies= True, split_point=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RJ0b3hUb3wjL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cGNVtRPb37JG"
   },
   "source": [
    "## 30 minute time block with 62 features\n",
    "$ vital sign measurements, flowsheets comments, order entry, nursing notes, occurrence of entities extracted from nursing notes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KCWsdaBs37JI"
   },
   "outputs": [],
   "source": [
    "tdata_name,tlabels_name,hdata_name,hlabels_name = \"tseq_30Tr_all.npy\", 'tlabels_30Tr_all.npy', \"hseq_30Tr_all.npy\", 'hlabels_30Tr_all.npy'\n",
    "\n",
    "data_training = np.load(\"gdrive/My Drive/\"+my_data_dir + tdata_name)\n",
    "labels_training = np.load(\"gdrive/My Drive/\"+my_data_dir + tlabels_name)\n",
    "\n",
    "data_holdout = np.load(\"gdrive/My Drive/\"+my_data_dir + hdata_name)\n",
    "labels_holdout = np.load(\"gdrive/My Drive/\"+my_data_dir + hlabels_name)\n",
    "\n",
    "#view data\n",
    "print(labels_training.mean(), labels_holdout.mean())\n",
    "print(data_training.shape, data_holdout.shape)\n",
    "\n",
    "#\n",
    "pos_data = data_training[labels_training != 0]\n",
    "neg_data = data_training[labels_training == 0]\n",
    "pos_data= np.sum(pos_data,axis=1)\n",
    "neg_data= np.sum(neg_data,axis=1)\n",
    "print(pd.DataFrame({'outcome':pos_data.mean(axis=0),'survival':neg_data.mean(axis=0)}))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2Hs64Fct37JK"
   },
   "outputs": [],
   "source": [
    "SimpleRegression(data_training,labels_training,data_holdout,labels_holdout, split_point=62)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZWJh5tvc37JN"
   },
   "outputs": [],
   "source": [
    "DNN(data_training,labels_training,data_holdout,labels_holdout, split_point=62)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kzDA6a1E37JR"
   },
   "outputs": [],
   "source": [
    "LSTM(data_training,labels_training,data_holdout,labels_holdout, jdummies= False, split_point=62)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MnrH7qhw37JW"
   },
   "outputs": [],
   "source": [
    "GRU(data_training,labels_training,data_holdout,labels_holdout, jdummies= False, split_point=62)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "smD_oAWDAx6X"
   },
   "outputs": [],
   "source": [
    "SimpleRegression_j(data_training,labels_training,data_holdout,labels_holdout,split_point=62)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Oh4sCd19Ax6d"
   },
   "outputs": [],
   "source": [
    "DNN_j(data_training,labels_training,data_holdout,labels_holdout, split_point=62)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jEjSs6O337Ja"
   },
   "outputs": [],
   "source": [
    "LSTM(data_training,labels_training,data_holdout,labels_holdout, jdummies= True, split_point=62)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "igcjPYV937Jd"
   },
   "outputs": [],
   "source": [
    "GRU(data_training,labels_training,data_holdout,labels_holdout, jdummies= True, split_point=62)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fJql7M_-37Jf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MOrQI5JdjupF"
   },
   "source": [
    "## 60 minute time block with 5 features\n",
    "* vital sign measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WFXKrkZfjupG"
   },
   "outputs": [],
   "source": [
    "tdata_name,tlabels_name,hdata_name,hlabels_name = \"tseq_60Tr5jd.npy\", 'tlabels_60Tr5jd.npy', \"hseq_60Tr5jd.npy\", 'hlabels_60Tr5jd.npy'\n",
    "\n",
    "data_training = np.load(\"gdrive/My Drive/\"+my_data_dir + tdata_name)\n",
    "labels_training = np.load(\"gdrive/My Drive/\"+my_data_dir + tlabels_name)\n",
    "\n",
    "data_holdout = np.load(\"gdrive/My Drive/\"+my_data_dir + hdata_name)\n",
    "labels_holdout = np.load(\"gdrive/My Drive/\"+my_data_dir + hlabels_name)\n",
    "\n",
    "#view data\n",
    "print(labels_training.mean(), labels_holdout.mean())\n",
    "print(data_training.shape, data_holdout.shape)\n",
    "\n",
    "#\n",
    "pos_data = data_training[labels_training != 0]\n",
    "neg_data = data_training[labels_training == 0]\n",
    "pos_data= np.sum(pos_data,axis=1)\n",
    "neg_data= np.sum(neg_data,axis=1)\n",
    "print(pd.DataFrame({'outcome':pos_data.mean(axis=0),'survival':neg_data.mean(axis=0)}))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AyJOPIQFjupJ"
   },
   "outputs": [],
   "source": [
    "SimpleRegression(data_training,labels_training,data_holdout,labels_holdout,split_point=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WiXXH8qujupL"
   },
   "outputs": [],
   "source": [
    "DNN(data_training,labels_training,data_holdout,labels_holdout, split_point=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rJ3EYAl0jupO"
   },
   "outputs": [],
   "source": [
    "LSTM(data_training,labels_training,data_holdout,labels_holdout, jdummies= False, split_point=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DlSgShTyjupR"
   },
   "outputs": [],
   "source": [
    "GRU(data_training,labels_training,data_holdout,labels_holdout, jdummies= False, split_point=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SyQgLaD7BIXj"
   },
   "outputs": [],
   "source": [
    "SimpleRegression_j(data_training,labels_training,data_holdout,labels_holdout,split_point=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZTXIr9knBIXo"
   },
   "outputs": [],
   "source": [
    "DNN_j(data_training,labels_training,data_holdout,labels_holdout, split_point=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5fzaw8DYjupW"
   },
   "outputs": [],
   "source": [
    "LSTM(data_training,labels_training,data_holdout,labels_holdout, jdummies= True, split_point=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7qXUZXM3jupa"
   },
   "outputs": [],
   "source": [
    "GRU(data_training,labels_training,data_holdout,labels_holdout, jdummies= True, split_point=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SZXB8KfZLhWz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EbyjNUCx4HYh"
   },
   "source": [
    "## 60 minute time block with 9 features\n",
    "† vital sign measurements, flowsheets comments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qTSSAQq64HYm"
   },
   "outputs": [],
   "source": [
    "tdata_name,tlabels_name,hdata_name,hlabels_name = \"tseq_60Tr10jd.npy\", 'tlabels_60Tr10jd.npy', \"hseq_60Tr10jd.npy\", 'hlabels_60Tr10jd.npy'\n",
    "\n",
    "data_training = np.load(\"gdrive/My Drive/\"+my_data_dir + tdata_name)\n",
    "labels_training = np.load(\"gdrive/My Drive/\"+my_data_dir + tlabels_name)\n",
    "\n",
    "data_holdout = np.load(\"gdrive/My Drive/\"+my_data_dir + hdata_name)\n",
    "labels_holdout = np.load(\"gdrive/My Drive/\"+my_data_dir + hlabels_name)\n",
    "\n",
    "#view data\n",
    "print(labels_training.mean(), labels_holdout.mean())\n",
    "print(data_training.shape, data_holdout.shape)\n",
    "\n",
    "#\n",
    "pos_data = data_training[labels_training != 0]\n",
    "neg_data = data_training[labels_training == 0]\n",
    "pos_data= np.sum(pos_data,axis=1)\n",
    "neg_data= np.sum(neg_data,axis=1)\n",
    "print(pd.DataFrame({'outcome':pos_data.mean(axis=0),'survival':neg_data.mean(axis=0)}))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "moiaseX94HYs"
   },
   "outputs": [],
   "source": [
    "SimpleRegression(data_training,labels_training,data_holdout,labels_holdout,split_point=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ewPrsvdG4HYz"
   },
   "outputs": [],
   "source": [
    "DNN(data_training,labels_training,data_holdout,labels_holdout, split_point=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OHGPsRng4HY1"
   },
   "outputs": [],
   "source": [
    "LSTM(data_training,labels_training,data_holdout,labels_holdout, jdummies= False, split_point=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YaAtXqV34HY4"
   },
   "outputs": [],
   "source": [
    "GRU(data_training,labels_training,data_holdout,labels_holdout, jdummies= False, split_point=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TvDr83FuA2E9"
   },
   "outputs": [],
   "source": [
    "reg_pred = SimpleRegression_j(data_training,labels_training,data_holdout,labels_holdout,split_point=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hWvYmxrkA2FB"
   },
   "outputs": [],
   "source": [
    "dnn_pred = DNN_j(data_training,labels_training,data_holdout,labels_holdout, split_point=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "taFvW_wq4HY9"
   },
   "outputs": [],
   "source": [
    "lstm_pred = LSTM(data_training,labels_training,data_holdout,labels_holdout, jdummies= True, split_point=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2WfWVdCk4HZA"
   },
   "outputs": [],
   "source": [
    "gru_pred = GRU(data_training,labels_training,data_holdout,labels_holdout, jdummies= True, split_point=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B5eRm5cdMioB"
   },
   "source": [
    "###Plot Precision-Recall Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-A8H9eN45qAd"
   },
   "outputs": [],
   "source": [
    "plot_pr_curve('Logistic Regression',labels_holdout, reg_pred, linestyle=':')\n",
    "plot_pr_curve('Deep Neural Network',labels_holdout, dnn_pred, linestyle='-.')\n",
    "plot_pr_curve('LSTM',labels_holdout, lstm_pred, linestyle='--')\n",
    "plot_pr_curve('GRU',labels_holdout, gru_pred, linestyle='-')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a7H-pZ7EE8nH"
   },
   "source": [
    "## 60 minutes time block with 15 features\n",
    "‡ vital sign measurements, flowsheets comments, order entry, nursing notes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1nlgD2MdE8nT"
   },
   "outputs": [],
   "source": [
    "tdata_name,tlabels_name,hdata_name,hlabels_name = \"tseq_60Tr15jd.npy\", 'tlabels_60Tr15jd.npy', \"hseq_60Tr15jd.npy\", 'hlabels_60Tr15jd.npy'\n",
    "\n",
    "data_training = np.load(\"gdrive/My Drive/\"+my_data_dir + tdata_name)\n",
    "labels_training = np.load(\"gdrive/My Drive/\"+my_data_dir + tlabels_name)\n",
    "\n",
    "data_holdout = np.load(\"gdrive/My Drive/\"+my_data_dir + hdata_name)\n",
    "labels_holdout = np.load(\"gdrive/My Drive/\"+my_data_dir + hlabels_name)\n",
    "\n",
    "#view data\n",
    "print(labels_training.mean(), labels_holdout.mean())\n",
    "print(data_training.shape, data_holdout.shape)\n",
    "\n",
    "#\n",
    "pos_data = data_training[labels_training != 0]\n",
    "neg_data = data_training[labels_training == 0]\n",
    "pos_data= np.sum(pos_data,axis=1)\n",
    "neg_data= np.sum(neg_data,axis=1)\n",
    "print(pd.DataFrame({'outcome':pos_data.mean(axis=0),'survival':neg_data.mean(axis=0)}))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UnWA1RYmE8na"
   },
   "outputs": [],
   "source": [
    "SimpleRegression(data_training,labels_training,data_holdout,labels_holdout, split_point=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lWVMkSuIE8nd"
   },
   "outputs": [],
   "source": [
    "DNN(data_training,labels_training,data_holdout,labels_holdout, split_point=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2rLuV3DeE8nh"
   },
   "outputs": [],
   "source": [
    "LSTM(data_training,labels_training,data_holdout,labels_holdout, jdummies= False, split_point=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rwL88MuXE8nl"
   },
   "outputs": [],
   "source": [
    "GRU(data_training,labels_training,data_holdout,labels_holdout, jdummies= False, split_point=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aEwuSX-bA4A3"
   },
   "outputs": [],
   "source": [
    "SimpleRegression_j(data_training,labels_training,data_holdout,labels_holdout,split_point=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Duhnr5nEA4A-"
   },
   "outputs": [],
   "source": [
    "DNN_j(data_training,labels_training,data_holdout,labels_holdout, split_point=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z-O-3D_PE8nq"
   },
   "outputs": [],
   "source": [
    "LSTM(data_training,labels_training,data_holdout,labels_holdout, jdummies= True, split_point=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uOPaOQIiE8nt"
   },
   "outputs": [],
   "source": [
    "GRU(data_training,labels_training,data_holdout,labels_holdout, jdummies= True, split_point=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AoAmZT0pE8nx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gmuJ2IZ8E8n0"
   },
   "source": [
    "## 60 minute time block with 62 features\n",
    "$ vital sign measurements, flowsheets comments, order entry, nursing notes, occurrence of entities extracted from nursing notes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bTgcqOAm4FSr"
   },
   "outputs": [],
   "source": [
    "tdata_name,tlabels_name,hdata_name,hlabels_name = \"tseq_60Tr_all.npy\", 'tlabels_60Tr_all.npy', \"hseq_60Tr_all.npy\", 'hlabels_60Tr_all.npy'\n",
    "\n",
    "data_training = np.load(\"gdrive/My Drive/\"+my_data_dir + tdata_name)\n",
    "labels_training = np.load(\"gdrive/My Drive/\"+my_data_dir + tlabels_name)\n",
    "\n",
    "data_holdout = np.load(\"gdrive/My Drive/\"+my_data_dir + hdata_name)\n",
    "labels_holdout = np.load(\"gdrive/My Drive/\"+my_data_dir + hlabels_name)\n",
    "\n",
    "#view data\n",
    "print(labels_training.mean(), labels_holdout.mean())\n",
    "print(data_training.shape, data_holdout.shape)\n",
    "\n",
    "#\n",
    "pos_data = data_training[labels_training != 0]\n",
    "neg_data = data_training[labels_training == 0]\n",
    "pos_data= np.sum(pos_data,axis=1)\n",
    "neg_data= np.sum(neg_data,axis=1)\n",
    "print(pd.DataFrame({'outcome':pos_data.mean(axis=0),'survival':neg_data.mean(axis=0)}))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6EBkdbZdE8n4"
   },
   "outputs": [],
   "source": [
    "SimpleRegression(data_training,labels_training,data_holdout,labels_holdout, split_point=62)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p2MQzgjlE8n8"
   },
   "outputs": [],
   "source": [
    "DNN(data_training,labels_training,data_holdout,labels_holdout, split_point=62)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s8fnOzNfE8n_"
   },
   "outputs": [],
   "source": [
    "LSTM(data_training,labels_training,data_holdout,labels_holdout, jdummies= False, split_point=62)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4UBYY30fE8oC"
   },
   "outputs": [],
   "source": [
    "GRU(data_training,labels_training,data_holdout,labels_holdout, jdummies= False, split_point=62)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6YUqxHNWA7C7"
   },
   "outputs": [],
   "source": [
    "SimpleRegression_j(data_training,labels_training,data_holdout,labels_holdout,split_point=62)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9a9sa4gPA7DA"
   },
   "outputs": [],
   "source": [
    "DNN_j(data_training,labels_training,data_holdout,labels_holdout, split_point=62)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ahaz15DTE8oF"
   },
   "outputs": [],
   "source": [
    "LSTM(data_training,labels_training,data_holdout,labels_holdout, jdummies= True, split_point=62)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mx9A8w4RE8oI"
   },
   "outputs": [],
   "source": [
    "GRU(data_training,labels_training,data_holdout,labels_holdout, jdummies= True, split_point=62)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VkezYsbOSbB5"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Copy of MAT_training_last3612_0514.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
